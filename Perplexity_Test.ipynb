{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Perplexity test on Llama GGUF model\n",
    "\n",
    "This script has been made and tested on Google Collab, it is highly advised to run it there on a GPU runtime\n",
    "\n",
    "To set it up fill the configuration constants at the beginning of the script layed out in upper case"
   ],
   "id": "a9c7b9438c9c64f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Installing dependencies and downloading model\")\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install -qqq --no-deps {xformers} trl peft accelerate bitsandbytes triton --progress-bar off\n",
    "from unsloth import FastLanguageModel\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "\n",
    "\n",
    "TEST_DATA_URL = \"\" #https://cosmo.zip/pub/datasets/wikitext-2-raw/wiki.test.raw\n",
    "REPO_ID = \"\"\n",
    "SELECTED_MANUAL_FILE_FROM_REPO= \"\" #FineLlama-3.1-8B-Q8_0.gguf\n",
    "\n",
    "\n",
    "\n",
    "# Compile llama.cpp with CUDA\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "%cd llama.cpp\n",
    "# Clean build directory if exists\n",
    "!rm -rf build\n",
    "!mkdir build\n",
    "%cd build\n",
    "# Set up CUDA support\n",
    "!cmake .. -DGGML_CUDA=ON\n",
    "# Build the Release configuration\n",
    "!cmake --build . --config Release -j 4\n",
    "%cd ../..\n",
    "\n",
    "\n",
    "try:\n",
    "    files = list_repo_files(REPO_ID)\n",
    "    # Filter .gguf files\n",
    "    gguf_files = [f for f in files if f.endswith('.gguf')]\n",
    "\n",
    "    if not gguf_files:\n",
    "        raise ValueError(\"No GGUF files found in the repository\")\n",
    "\n",
    "    # Prefer Q8_0, otherwise take the first one\n",
    "    selected_file = next((f for f in gguf_files if \"Q8_0\" in f), gguf_files[0])\n",
    "\n",
    "    # Download model\n",
    "    model_path = hf_hub_download(repo_id=REPO_ID, filename=selected_file)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error locating/downloading model: {e}\")\n",
    "    # Fallback to manual definition if list fails\n",
    "    model_path = hf_hub_download(repo_id=REPO_ID, filename=SELECTED_MANUAL_FILE_FROM_REPO)\n",
    "\n",
    "# Download test data\n",
    "!wget -O wiki.test.raw {TEST_DATA_URL}\n",
    "\n",
    "print(\"Running Perplexity Test...\")\n",
    "\n",
    "# The binary location might vary slightly depending on cmake version\n",
    "perplexity_bin = \"./llama.cpp/build/bin/llama-perplexity\"\n",
    "\n",
    "# Arguments:\n",
    "# -m: Model path\n",
    "# -f: Input file\n",
    "# -ngl 99: Offload 99 layers to GPU\n",
    "# -c 2048: Context size\n",
    "cmd = f\"{perplexity_bin} -m '{model_path}' -f wiki.test.raw -ngl 99 -c 2048\"\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "print(\"Benchmark complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
